# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_4w8rTZYgUnaUXjksFGR4Sm9EoUDN1oi
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from sklearn.feature_extraction.text import CountVectorizer
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.optimizers import Adam

from google.colab import drive
drive.mount('/content/drive')

review_datasetpath = '/content/drive/MyDrive/yelp_dataset/yelp_academic_dataset_review.json'

# Load review data
review_df = pd.read_json(review_datasetpath, lines=True, nrows=1000000)

print("Review DataFrame Info:")
print(review_df.info())

print("Sample of Review DataFrame:")
print(review_df.head())

# Create a new column "length" with the word length of the review
review_df['length'] = review_df['text'].apply(lambda x: len(x.split()))

# Visualize the correlation between stars and the length of the review
sns.scatterplot(x='stars', y='length', data=review_df)
plt.title('Correlation between Stars and Review Length')
plt.show()

# getting mean value of the vote columns
mean_votes = review_df[['useful', 'funny', 'cool']].mean()
print('Mean Value of the Vote columns:')
print(mean_votes)

# Correlation between the voting columns
correlation_matrix = review_df[['useful', 'funny', 'cool']].corr()
print('\nCorrelation between the voting columns:')
print(correlation_matrix)

def preprocess_text(text):
    text = text.lower()
    text = ' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])
    text = ''.join([char for char in text if char.isalnum() or char.isspace()])
    return text


# Apply text preprocessing to the 'text' column in the review DataFrame
review_df['text'] = review_df['text'].apply(preprocess_text)

# Handle missing values if any
review_df.dropna(inplace=True)

# Remove duplicate rows based on the 'text' column
review_df.drop_duplicates(subset=['text'], inplace=True)

# Assuming sentiment is positive for stars greater than or equal to 4, and negative otherwise
review_df['label'] = (review_df['stars'] >= 4).astype(int)

# Display the preprocessed DataFrame
print("\nSample of Preprocessed Review DataFrame:")
print(review_df.head())

# Save the preprocessed review DataFrame to a CSV file
preprocessed_csv_path = '/content/drive/MyDrive/yelp_dataset/preprocessed_reviews.csv'
review_df.to_csv(preprocessed_csv_path, index=False)
print(f"Preprocessed dataset saved to {preprocessed_csv_path}")

train_data, test_data = train_test_split(review_df, test_size=0.2, random_state=42)

# Load  preprocessed Yelp Reviews dataset
data = pd.read_csv('/content/drive/MyDrive/yelp_dataset/preprocessed_reviews.csv')

# getting data to list from the lable column for sentiment (0 or 1)
labels = data['label'].tolist()

distilbert_model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

# Tokenize and preprocess the training data
train_reviews = train_data['text'].tolist()
train_labels = train_data['label'].tolist()
train_inputs = tokenizer(train_reviews, padding=True, truncation=True, return_tensors='tf', max_length=128)
train_inputs_numpy = {
    'input_ids': np.array(train_inputs['input_ids']),
    'attention_mask': np.array(train_inputs['attention_mask'])
}
train_labels = tf.convert_to_tensor(train_labels)

# Tokenize and preprocess the test data
test_reviews = test_data['text'].tolist()
test_labels = test_data['label'].tolist()
test_inputs = tokenizer(test_reviews, padding=True, truncation=True, return_tensors='tf', max_length=128)
test_inputs_numpy = {
    'input_ids': np.array(test_inputs['input_ids']),
    'attention_mask': np.array(test_inputs['attention_mask'])
}
test_labels = tf.convert_to_tensor(test_labels)

batch_size = 32
distilbert_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
distilbert_model.fit(train_inputs_numpy, train_labels, epochs= 1 , batch_size=batch_size, validation_data=(test_inputs_numpy, test_labels))

distilbert_test_loss, distilbert_test_accuracy = distilbert_model.evaluate(test_inputs_numpy, test_labels)
print(f'DistilBERT Model Test Accuracy: {distilbert_test_accuracy * 100:.2f}%')

distilbert_predictions = distilbert_model.predict(test_inputs)
distilbert_predictions = np.argmax(distilbert_predictions, axis=1)
distilbert_precision = precision_score(test_data['label'], distilbert_predictions)
distilbert_recall = recall_score(test_data['label'], distilbert_predictions)
distilbert_f1 = f1_score(test_data['label'], distilbert_predictions)
distilbert_conf_matrix = confusion_matrix(test_data['label'], distilbert_predictions)

print(f'DistilBERT Model Precision: {distilbert_precision:.2f}')
print(f'DistilBERT Model Recall: {distilbert_recall:.2f}')
print(f'DistilBERT Model F1 Score: {distilbert_f1:.2f}')
print('DistilBERT Model Confusion Matrix:')
print(distilbert_conf_matrix)